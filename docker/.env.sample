# =============================================================================
# siem_ai  —  Environment Configuration
# Copy this file to docker/.env and edit as needed.
# =============================================================================

# -----------------------------------------------------------------------------
# SIEM / Source Type
# auto     = auto-detect format per request (recommended for mixed environments)
# splunk   = treat all /ingest/webhook POSTs as Splunk HEC
# elastic  = treat all /ingest/webhook POSTs as Elastic/ECS
# generic  = plain JSON wrap (works with Wazuh, Graylog, any JSON webhook)
# -----------------------------------------------------------------------------
SIEM_TYPE=auto

# -----------------------------------------------------------------------------
# AI Model (playbook generation backend)
# local  = rule-based template selection (no LLM required, works offline)
# remote = call an external LLM endpoint (set LLM_ENDPOINT + API_KEY below)
# -----------------------------------------------------------------------------
MODEL_MODE=local
LLM_ENDPOINT=
# LLM_ENDPOINT=http://your-ollama-host:11434/api/generate   # Ollama example
# LLM_ENDPOINT=https://api.openai.com/v1/chat/completions   # OpenAI example

# -----------------------------------------------------------------------------
# Authentication  (leave blank to run in lab/demo mode with no auth)
# In production: generate with  python3 -c "import secrets; print(secrets.token_hex(32))"
# Send as header:  X-API-Key: <value>
# -----------------------------------------------------------------------------
API_KEY=

# -----------------------------------------------------------------------------
# HTTP Webhook  (receives JSON from Splunk HEC, Elastic, Wazuh, Graylog, etc.)
# -----------------------------------------------------------------------------
WEBHOOK_PORT=5000

# -----------------------------------------------------------------------------
# Syslog UDP  (RFC 3164 / RFC 5424 / CEF / LEEF — auto-detected per packet)
# Standard port is 514; use 1514 if running rootless (ports < 1024 need root)
# Point your SIEM's syslog output here.
# -----------------------------------------------------------------------------
SYSLOG_UDP_PORT=514

# -----------------------------------------------------------------------------
# Syslog TCP  (same auto-detection; RFC 3195 standard port is 601)
# rsyslog / syslog-ng default TCP port is 514; set to match your SIEM.
# Set to 0 to disable TCP listener.
# -----------------------------------------------------------------------------
SYSLOG_TCP_PORT=601

# -----------------------------------------------------------------------------
# Review UI
# -----------------------------------------------------------------------------
REVIEW_UI_PORT=8088

# -----------------------------------------------------------------------------
# Rate limiting  (per source IP)
# -----------------------------------------------------------------------------
RATE_LIMIT=100 per minute

# -----------------------------------------------------------------------------
# Logging
# DEBUG | INFO | WARNING | ERROR
# -----------------------------------------------------------------------------
LOG_LEVEL=INFO

# -----------------------------------------------------------------------------
# Queue poll interval (seconds) — how often the AI generator checks for new events
# -----------------------------------------------------------------------------
POLL_INTERVAL=2
